---
author: jckuhl
comments: true
date: 2018-02-05 20:20:26+00:00
layout: post
link: https://projectbreakpoint.wordpress.com/2018/02/05/digital-literacy/
slug: digital-literacy
title: Digital Literacy
wordpress_id: 103
categories:
- programming
tags:
- Algorithms
- digital literacy
---

I work at a call center for a credit card.  One of our major partners is a Maine outdoorsy store known as L. L. Bean.  The customers who use the L. L. Bean visa card tend to be in their sixties or seventies if not older.

I, on the other hand, am about the same age as the Internet itself.  I grew up as the Internet grew up.  I watched first hand as processors got smaller and smaller, while growing more and more powerful.  I remember a time when 256 megabytes was "a lot" of space.

When I'm at work I can clearly see the generational gap between myself and the older L. L. Bean customers.  If I suggest they use the webpage, many of them tend to balk.  If I let them know they can make payments on the mobile app, they inform me that they still use a landline with actual physical buttons and a curly cord.  Many of them either mistrust, or simply don't use, modern technology.

And who can blame them?  They spent the first two thirds of their life without these technologies that we're now sliding into our pockets every day.  In their time, computers were big and bulky and not very useful outside of serious industry and the occasional hobbyist.  Computers were, at one time, not one simple little machine you place on a desk, but massive groups of interconnected consoles with punch cards and magnetic tapes that cost thousands of dollars to set up, thousands more to use, and took up an entire room or two.  They weren't for everyday people.

But that era has ended.  It's not uncommon for people to have three or more computers.  I have a laptop, an iPad and a smartphone.  These things are everywhere now.  In this day and age, everyone should know computer basics and, I argue, everyone should know how to code.  Programming should be taught in elementary schools alongside computer classes.

There are multiple reasons for this.

First and foremost is security.  Because everyone and their mother has access to computers and the Internet, it means that cybercrime is on the rise, a lot of which can be avoided by taking very basic precautions.  If you understand how packets travel through the internet from client to server and back, you'll have a better appreciation for the security steps to secure that information.

I get a lot of comments from the less-computer savvy while at work about how people don't want to do their banking online, how they fear that their information will get compromised and granted, with Equifax failing colossally at their job, that worry is not without cause.  However, having a basic understand of how things like HTTPS works, how certificates work, how encryption and hashing all work, you'll understand why banking and other financial activities are generally risk free.  It'll also help you avoid shady and scammy sites.

With everyone's data being passed and forth through ethernet cables and wifi signals, it's vital to ensure that your data cannot be picked up by someone else.  For example, someone could be sitting outside with a packet sniffer reading the WiFi signal.  Packet sniffers are dangerous, but only if the data is poorly encrypted.  Average consumers need to know at least the basics for what sort of security to look for when on the Internet, so they avoid falling victim to attacks.

Secondly, encouraging digital literacy will help children and young adults flourish and find their place in this world.  Introducing them to MIT's scratch or Lego's Mindstorms, or even Python's turtle module might inspire more of them to go into STEM fields.  By showing them how they can use computers to explore and create, they can be motivated to go further.  Exposing them to these concepts can set many of them on track to achieve great things later on in their lives.

Technology is everywhere.  I've heard it said, and I think it was by Linus Torvalds, that technological opportunities in tech fields aren't going up; technological opportunities in _all_ fields are going up.  Every business has a website.  Every business has a database.  Many businesses might want to use their own software or manage their own social media presence.  Everyone wants their own app.  While automation is coming along and cutting away job fields in other industries, everyone is in need of software engineers and web developers.

Third, there's more to computer science to writing code.  After all, there isn't too much difference between C++, Python, JavaScript, GoLang, Ruby, except for syntax and specific features.   If you know the basics of one language, you know the basics of all languages.  C++ might have pointer arithmetic whereas JavaScript and Python don't even have pointers, but that's a detail.  Programming languages are about problem solving and algorithms, and those concepts, at a high level, don't really change from language to language; only the specific implementation line by line does.  A linked list is a linked list.  It might be syntactically different if written in C++ than in Pythin, but the concept of the linked list doesn't change.  The idea of using a linked list to solve a problem does not change.

Think of a language that we speak with.  The concept of French and English are both the same.  You put words in some sort of order (known as grammar, or syntax) to express an idea that someone else can understand.  If you know how to talk, you languages.  Being an English speaker who's learning french is not a matter of learning how to talk, it's just a matter of adjusting to a different vocabulary and grammatical structure.  The same applies for computer programming languages  At their core, they aren't about syntax so much as they are about solving a problem.

Being able to look at things algorithmically is a skill that allows you to look at a problem and come to a solution piece by piece.  Just like in programming, you construct your solution primitive by primitive, and just like in programming, when something fails, you consider why it failed and implement a fix.  Programming teaches people to think critically, to analyze and debug their solutions to real life problems and to apply logic to these problems.

Furthermore, it also teaches people to fail.  Failure is a part of life as much as it is a part of coding.  It takes many iterations of me working on a piece of code to get something to work.  I'm often running around on [Stack Overflow](https://stackoverflow.com/) or the [Mozilla Developer Network](https://developer.mozilla.org/en-US/) looking for a solution.  Failure is vital, it teaches us what works and what doesn't.  It lets us grow, and lets us learn.

Will Smith said it best:


[youtube https://www.youtube.com/watch?v=wFf6rhcYkXw&w=560&h=315]


Those who learn from their failures only get stronger. Failure is the only way you can grow. Success is great, don't get me wrong. But you'll fail more than you succeed, and you'll fail so that you'll succeed.

Finally, and I touched on this earlier, coding leads to resourcefulness.  No one has all the answers.  No one memorizes every single piece of code, every single data structure, every single method.  CSS has hundreds of different selectors.  JavaScript has dozens of different Array.prototype methods.  There's no problem solved by exactly one algorithm, and it takes research to determine how to implement the best one.  Coding teaches you to use your resources, to crack open books or to conference with your peers.

Programming is no longer some arcane and esoteric exercise that only highly trained nerds are capable of.  It is an increasingly necessary skill.  As a hard skill, it is valued in every industry in today's digital age and as a soft skill, it can extend to non technical fields by teaching people how to solve problems and find answers.  In our technologically advanced world, it's time we make these skills a priority in our classrooms.

Happy coding.
